2025/04/14  -  18:10
status: 

Tags: [[AI]]  

Is a phenomenon in modern machine learning where models can fit noisy training data (overfit) yet achieve good generalization performance on unseen test data. It can happen especially in large models ( deep networks ). The model has to have more parameters than the data, also it works better in finite-dimensional data. overfitting can be classified into three categories: 
1. Benign overfitting
2. Tempered overfitting: noise in training data can cause non zero excess risk at test time. 
3. Catastrophic overfitting: tradition overfitting.
Key conditions for benign overfitting:
4. overparameterization:
5. Signal to noise ratio:
6. Data distribution properties:
7. Model architecture:


# References
